{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "class SuppressStdout:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        sys.stderr = self._original_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/root/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"/root/PediatricDrugDoses.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "with SuppressStdout():\n",
    "    vectordb = Chroma.from_documents(documents=pages, embedding=embeddings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0,\n",
    "    top_k=10,\n",
    "    top_p=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are healthcare Assistant who suggests pediatric drug dosage.  \n",
    "Human will give you the symptoms that he/she is experiencing.\n",
    "You will be provided with a context from our database that contains information relevant to the symptoms described by the Human.\n",
    "Also analyze the context if the mentioned symptoms are there in it. If not, ask a relevant questions like \"Tell me more about the symptoms\"\n",
    "If you are not sure that it is the right dosage, ask questions related to the context to be 100 percent sure.\n",
    "If you are still not sure after asking all the questions, simply respond with \"I don't know. Can you ask another question\".\n",
    "If questions are asked where there is no relevant context available, simply respond with \"I don't know. Please ask a question relevant to the documents\"\n",
    "You'll also get the chat history, so that you can collect all the symptoms of the user and make a clear decision. Make sure you do not ask the same questions again.\n",
    "Once you are sure of the dosage, suggest it to the human and do not ask additional questions, ignore the context.\n",
    "Chat history: {chat_history}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Human: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"], template=template\n",
    ")\n",
    "\n",
    "# Create the custom chain\n",
    "if llm is not None and vectordb is not None:\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        retriever=vectordb.as_retriever(search_type='mmr', search_kwargs={'k':2, 'fetch_k': 50, 'lambda_mult': 0.2}), \n",
    "        memory=memory,\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True,\n",
    "        combine_docs_chain_kwargs={'prompt': prompt})\n",
    "else:\n",
    "    print(\"LLM or Vector Database not initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Patient is having vommiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"I am having Fever\"\n",
    "# all_messages = \"\"\n",
    "# for memid,message in enumerate(memory.chat_memory.messages):\n",
    "#     if memid%2 != 0:\n",
    "#         message = message[memid]\n",
    "#     all_messages += message\n",
    "    \n",
    "# chain.invoke(input+\"<PREV_MESSAGES>\"+all_messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
